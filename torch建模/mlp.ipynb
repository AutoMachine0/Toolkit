{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the epoch:1 loss: 950.8309353590012\n",
      "the epoch:2 loss: 917.8038918972015\n",
      "the epoch:3 loss: 908.8398736715317\n",
      "the epoch:4 loss: 903.2410755157471\n",
      "the epoch:5 loss: 900.4795634746552\n",
      "the epoch:6 loss: 897.5223001241684\n",
      "the epoch:7 loss: 895.9417276382446\n",
      "the epoch:8 loss: 893.301366686821\n",
      "the epoch:9 loss: 891.5964286327362\n",
      "the epoch:10 loss: 891.133666396141\n",
      "test performace, 0.9687000089883804\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision as tv\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\"\"\"\n",
    "1.torch.nn / torch.nn.functional:\n",
    "(1).__init__函数初始化使用 torch.nn\n",
    "(2).forward计算使用 torch.nn.functional\n",
    "\n",
    "2.torch.nn.Sequentail()\n",
    "构建模型存储序列\n",
    "\n",
    "3.基于torch.nn.Sequentail()动态加载torch模块\n",
    "self.mlp = torch.nn.Sequential()\n",
    "self.mlp.add_module（model_name(str),module）\n",
    "\n",
    "4.torch中的优化函数与损失函数需要实例化后可正常使用\n",
    "# select optimizer function\n",
    "self.optim_f = self.get_optimization_function(opt)(self.mlp.parameters(), lr=0.001)  # 优化函数实例化\n",
    "# select loss function\n",
    "self.loss_f = self.get_loss_function(loss)()                                         # 损失函数实例化\n",
    "\n",
    "5.torch.util.data.TensorDataset(x, y)\n",
    "(1).TensorDataset 对 x 与 y 进行数据打包,类似zip功能\n",
    "(2).x,y tensor第一个维度必须相同\n",
    "\n",
    "6.torch.utils.data.DataLoader(TensorDataset,batch_size=10)\n",
    "(1).与TensorDataset连用,将TensorDataset类型的数进行分批次\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "class MLP(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 output_dim,\n",
    "                 hidden_dim_list=[],\n",
    "                 activate_list=[\"relu\"],\n",
    "                 opt=\"adam\",\n",
    "                 loss='CrossEntropyLoss'):\n",
    "\n",
    "        super(MLP, self).__init__()\n",
    "        self.mlp = torch.nn.Sequential()\n",
    "\n",
    "        # construct mlp model\n",
    "        if (len(hidden_dim_list) == 0):\n",
    "            self.mlp.add_module(\"linear_layer 1\", torch.nn.Linear(input_dim, output_dim))\n",
    "            if activate_list[0] == \"linear\":\n",
    "                pass\n",
    "            else:\n",
    "                self.mlp.add_module(\"linear_layer 1 act\", self.get_act(activate_list[0]))\n",
    "\n",
    "        for i in range(len(hidden_dim_list)):\n",
    "            if i == 0:\n",
    "                self.mlp.add_module(\"linear_layer \" + str(i + 1), torch.nn.Linear(input_dim, hidden_dim_list[i]))\n",
    "                if activate_list[i] == \"linear\":\n",
    "                    pass\n",
    "                else:\n",
    "                    self.mlp.add_module(\"linear_layer \" + str(i + 1) + \"act\", self.get_act(activate_list[i]))\n",
    "            else:\n",
    "                self.mlp.add_module(\"linear_layer \" + str(i + 1),\n",
    "                                    torch.nn.Linear(hidden_dim_list[i - 1], hidden_dim_list[i]))\n",
    "                if activate_list[i] == \"linear\":\n",
    "                    pass\n",
    "                else:\n",
    "                    self.mlp.add_module(\"linear_layer \" + str(i + 1) + \"act\", self.get_act(activate_list[i]))\n",
    "\n",
    "        if (len(hidden_dim_list) != 0):\n",
    "            self.mlp.add_module(\"linear_layer \" + str(len(hidden_dim_list) + 2), torch.nn.Linear(hidden_dim_list[i], output_dim))\n",
    "\n",
    "        # select optimizer function\n",
    "        self.optim_f = self.get_optimization_function(opt)(self.mlp.parameters(), lr=0.001)  # 优化函数实例化\n",
    "        # select loss function\n",
    "        self.loss_f = self.get_loss_function(loss)()                                         # 损失函数实例化\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output = self.mlp(x)\n",
    "\n",
    "        pred_y = F.softmax(output, dim=1)\n",
    "\n",
    "        return pred_y\n",
    "\n",
    "    def get_optimization_function(self, opt):\n",
    "\n",
    "        optim_f = None\n",
    "\n",
    "        if opt == \"sgd\":                       # 随机梯度下降\n",
    "            optim_f = torch.optim.SGD\n",
    "        elif opt == \"asgd\":                    # 平均随机梯度下降\n",
    "            optim_f = torch.optim.ASGD\n",
    "        elif opt == \"adagrad\":\n",
    "            optim_f = torch.optim.Adagrad\n",
    "        elif opt == \"adadelta\":\n",
    "            optim_f = torch.optim.Adadelta\n",
    "        elif opt == \"rmsprop\":\n",
    "            optim_f = torch.optim.RMSprop\n",
    "        elif opt == \"adam\":\n",
    "            optim_f = torch.optim.Adam\n",
    "        elif opt == \"adamax\":\n",
    "            optim_f = torch.optim.Adamax\n",
    "        elif opt == \"sparseadam\":\n",
    "            optim_f = torch.optim.SparseAdam\n",
    "        elif opt == \"lbfgs\":\n",
    "            optim_f = torch.optim.LBFGS\n",
    "\n",
    "        return optim_f\n",
    "\n",
    "    def get_loss_function(self, loss):\n",
    "\n",
    "        loss_f = None\n",
    "        # 多分类损失函数\n",
    "        if loss == \"CrossEntropyLoss\":          # 交叉熵损失\n",
    "            loss_f = torch.nn.CrossEntropyLoss\n",
    "        elif loss == \"NLLLoss\":                 # 负对数似然损失\n",
    "            loss_f = torch.nn.NLLLoss\n",
    "\n",
    "        return loss_f\n",
    "\n",
    "    def get_act(self, act):\n",
    "\n",
    "        if act == \"elu\":\n",
    "            act = torch.nn.ELU()\n",
    "        elif act == \"leaky_relu\":\n",
    "            act = torch.nn.LeakyReLU()\n",
    "        elif act == \"relu\":\n",
    "            act = torch.nn.ReLU()\n",
    "        elif act == \"relu6\":\n",
    "            act = torch.nn.ReLU6()\n",
    "        elif act == \"sigmoid\":\n",
    "            act = torch.nn.Sigmoid()\n",
    "        elif act == \"softplus\":\n",
    "            act = torch.nn.Softplus()\n",
    "        elif act == \"tanh\":\n",
    "            act = torch.nn.Tanh()\n",
    "        else:\n",
    "            raise (\"wrong act type:\", act)\n",
    "\n",
    "        return act\n",
    "\n",
    "def model_train(model,dataset,train_epoch):\n",
    "    train_set = None\n",
    "\n",
    "    if \"train_set\" in dataset.keys():\n",
    "        train_set = dataset[\"train_set\"]\n",
    "\n",
    "    for epoch in range(train_epoch):\n",
    "\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for i, batch_train_data in enumerate(train_set):\n",
    "\n",
    "            model.optim_f.zero_grad()  # 每一个batch训练前清空梯度\n",
    "\n",
    "            x, y = batch_train_data\n",
    "\n",
    "            x = torch.autograd.Variable(x).cuda()\n",
    "            y = torch.autograd.Variable(y).cuda()\n",
    "\n",
    "            pred_y = model(x)\n",
    "            loss = model.loss_f(pred_y, y)\n",
    "            loss.backward()\n",
    "            model.optim_f.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print('the epoch:' + str(epoch+1) + ' loss:', train_loss)\n",
    "\n",
    "    return model\n",
    "\n",
    "def model_test(model, dataset):\n",
    "\n",
    "    if \"test_set\" in dataset.keys():\n",
    "        test_set = dataset[\"test_set\"]\n",
    "    performance = 0.0\n",
    "\n",
    "    for i, data in enumerate(test_set):\n",
    "\n",
    "        x, y = data\n",
    "\n",
    "        x = torch.autograd.Variable(x).cuda()\n",
    "        y = torch.autograd.Variable(y).cuda()\n",
    "\n",
    "        pred_y = model(x)\n",
    "        performance += evaluate(pred_y, y)\n",
    "\n",
    "    performance = performance/(i+1)\n",
    "\n",
    "    print(\"test performace,\", performance)\n",
    "\n",
    "\n",
    "def get_data():\n",
    "\n",
    "    current_path = os.getcwd()\n",
    "    train_set = tv.datasets.MNIST(current_path + \"/mnist/train\",\n",
    "                                  train=True,\n",
    "                                  transform=tv.transforms.ToTensor(),\n",
    "                                  download=True)\n",
    "\n",
    "    train_x = train_set.data.view(-1, 28 * 28).float()/255\n",
    "    train_y = train_set.targets.to(torch.int64)\n",
    "    train_set = Data.TensorDataset(train_x, train_y)\n",
    "\n",
    "    test_set = tv.datasets.MNIST(current_path + \"/mnist/test\",\n",
    "                                 train=False,\n",
    "                                 transform=tv.transforms.ToTensor(),\n",
    "                                 download=True)\n",
    "\n",
    "    test_x = test_set.data.view(-1, 28 * 28).float()/255\n",
    "    test_y = test_set.targets.to(torch.int64)\n",
    "    test_set = Data.TensorDataset(test_x, test_y)\n",
    "\n",
    "    train_dataset = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "    test_dataset = torch.utils.data.DataLoader(test_set, batch_size=100)\n",
    "    \n",
    "    input_dim = 784\n",
    "    output_dim = 10\n",
    "\n",
    "    return train_dataset, test_dataset, input_dim, output_dim\n",
    "\n",
    "def evaluate(pred, y):\n",
    "\n",
    "    pred = pred.cpu().data.numpy()\n",
    "    label = y.cpu().data.numpy()\n",
    "    test_np = (np.argmax(pred, 1) == label)\n",
    "    test_np = np.float32(test_np)\n",
    "    return np.mean(test_np)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    train_dataset, test_dataset, input_dim, output_dim = get_data()\n",
    "\n",
    "    data_set = {\"train_set\": train_dataset,\n",
    "                \"test_set\": test_dataset}\n",
    "\n",
    "    mlp = MLP(input_dim, output_dim, hidden_dim_list=[512, 128], activate_list=[\"elu\", \"elu\"]).cuda()\n",
    "\n",
    "    model = model_train(mlp, data_set, 10)\n",
    "\n",
    "    model_test(model, data_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
